{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V33D9Ax7Xt_j"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv('data/name_gender_filtered.csv')\n",
        "unique_chars = set()\n",
        "\n",
        "for name in df['Name']:\n",
        "    unique_chars.update(name)\n",
        "    \n",
        "sorted_chars = sorted(list(unique_chars))\n",
        "sorted_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zdT1nqeXxHb",
        "outputId": "c1e79434-1d66-4d8e-eadb-930141a41458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '<S>', 27: '<E>'}\n"
          ]
        }
      ],
      "source": [
        "sorted_chars = sorted(set(''.join(sorted_chars)))\n",
        "stoi = {s:i for i,s in enumerate(sorted_chars)}\n",
        "stoi['<S>'] = len(stoi)\n",
        "stoi['<E>'] = len(stoi)\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU9DMZl7X0Fp",
        "outputId": "e902b35e-6b56-40a5-ced8-915c7499a27a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "torch.Size([5, 28])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def name_to_one_hot(name):\n",
        "    # Add start and end tokens to the name\n",
        "    tokenized_name = ['<S>'] + list(name) + ['<E>']\n",
        "    int_tensor = torch.tensor([stoi[char] for char in tokenized_name])\n",
        "    one_hot_encoded = F.one_hot(int_tensor, num_classes=len(stoi)).float()\n",
        "    return one_hot_encoded\n",
        "\n",
        "\n",
        "print(name_to_one_hot(\"leo\"))\n",
        "print(name_to_one_hot(\"leo\").size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8GyYBxxMX3Pu"
      },
      "outputs": [],
      "source": [
        "from myRNN import *\n",
        "\n",
        "n_letters = len(stoi)\n",
        "n_hidden = 1024\n",
        "rnn_model = MyRNN(n_letters, n_hidden, n_letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled index: 0\n",
            "Sampled index: 0\n",
            "Sampled index: 2\n",
            "Sampled index: 0\n",
            "Sampled index: 0\n",
            "Sampled index: 0\n",
            "Sampled index: 0\n",
            "Sampled index: 0\n",
            "Sampled index: 0\n",
            "Sampled index: 1\n"
          ]
        }
      ],
      "source": [
        "prob = torch.tensor([0.7, 0.2, 0.1])\n",
        "\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")\n",
        "print(f\"Sampled index: {torch.multinomial(prob, 1).item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_-hMA7FX4jQ",
        "outputId": "8188d89c-107d-4b11-c839-33b8af5485fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<S>qatbzpwc\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def generate_name():\n",
        "    rnn_model.eval()\n",
        "    start_token_idx = torch.tensor(stoi['<S>'])\n",
        "    one_hot_encoded = F.one_hot(start_token_idx, num_classes=len(stoi)).float()\n",
        "    hidden = rnn_model.get_hidden()\n",
        "    char_list = []\n",
        "    for i in range(20):\n",
        "        out_score, hidden = rnn_model(one_hot_encoded[None,:],hidden)\n",
        "        score_probability = F.softmax(out_score[0], dim=-1)\n",
        "        out_idx = torch.multinomial(score_probability, 1).item()\n",
        "        if out_idx == stoi['<E>']:\n",
        "            break\n",
        "        char_list.append(itos[out_idx])\n",
        "        one_hot_encoded = F.one_hot(torch.tensor(out_idx), num_classes=len(stoi)).float()\n",
        "    print(''.join(char_list))\n",
        "\n",
        "\n",
        "generate_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DZw6zHxRYA05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hyungseok/anaconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/hyungseok/Desktop/src/PL_deep_learning_fundamentals/08_rnn/03_nameGen.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyungseok/Desktop/src/PL_deep_learning_fundamentals/08_rnn/03_nameGen.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     target_char \u001b[39m=\u001b[39m name_one_hot[char_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyungseok/Desktop/src/PL_deep_learning_fundamentals/08_rnn/03_nameGen.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     target_class \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(target_char, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hyungseok/Desktop/src/PL_deep_learning_fundamentals/08_rnn/03_nameGen.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     out_score, hidden \u001b[39m=\u001b[39m rnn_model(input_tensor[\u001b[39mNone\u001b[39;49;00m,:],hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyungseok/Desktop/src/PL_deep_learning_fundamentals/08_rnn/03_nameGen.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss_fn(out_score[\u001b[39m0\u001b[39m], target_class))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyungseok/Desktop/src/PL_deep_learning_fundamentals/08_rnn/03_nameGen.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses)\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/src/PL_deep_learning_fundamentals/08_rnn/myRNN.py:15\u001b[0m, in \u001b[0;36mMyRNN.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hidden):\n\u001b[0;32m---> 15\u001b[0m     combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((\u001b[39minput\u001b[39;49m, hidden), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m     hidden \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi2h(combined))\n\u001b[1;32m     17\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh2o(hidden)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(rnn_model.parameters(), lr=0.0001)\n",
        "\n",
        "rnn_model.train()\n",
        "for epoch_idx in range(100):\n",
        "    shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
        "    crnt_loss = 0.\n",
        "    for index, row in shuffled_df.iterrows():\n",
        "\n",
        "        name_one_hot = name_to_one_hot(row['Name'])\n",
        "        hidden = rnn_model.get_hidden()\n",
        "        rnn_model.zero_grad()\n",
        "\n",
        "        losses = []\n",
        "        for char_idx in range(len(name_one_hot)-1):\n",
        "            input_tensor = name_one_hot[char_idx]\n",
        "            target_char = name_one_hot[char_idx+1]\n",
        "            target_class = torch.argmax(target_char, -1)\n",
        "            out_score, hidden = rnn_model(input_tensor[None,:],hidden)\n",
        "            losses.append(loss_fn(out_score[0], target_class))\n",
        "        loss = sum(losses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        crnt_loss += loss.item()\n",
        "\n",
        "    generate_name()\n",
        "    average_loss = crnt_loss / len(df)\n",
        "\n",
        "    print(f'Iter idx {epoch_idx}, Loss: {average_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imOpXEMxYC-q",
        "outputId": "28ccca00-02b9-4080-e8d8-9b07c6cc616a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ceairi\n",
            "lewnor\n",
            "valgwin\n",
            "shala\n",
            "ezmera\n",
            "raffamau\n",
            "kia\n",
            "jenavieve\n",
            "carlette\n",
            "sheily\n"
          ]
        }
      ],
      "source": [
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n",
        "generate_name()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
